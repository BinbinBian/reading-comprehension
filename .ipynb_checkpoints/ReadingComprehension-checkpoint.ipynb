{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import utils\n",
    "import os\n",
    "from collections import Counter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = '/Users/kellyzhang/Documents/ReadingComprehension/DeepMindDataset/cnn/questions'\n",
    "train_path = os.path.join(data_path, \"training\") # 380298\n",
    "validation_path = os.path.join(data_path, \"validation\") # 3924\n",
    "test_path = os.path.join(data_path, \"test\") #3198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_data_files(in_file_path, max_example=None, relabeling=True, write_file=None):\n",
    "    \"\"\"\n",
    "    load CNN / Daily Mail data from {train | dev | test} directories\n",
    "    relabeling: relabel the entities by their first occurence if it is True.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    num_examples = 0\n",
    "\n",
    "    for document in os.listdir(in_file_path):\n",
    "        f = open(os.path.join(in_file_path, document), 'r')\n",
    "\n",
    "        content = f.read().splitlines()\n",
    "        document = content[2].strip().lower()\n",
    "        question = content[4].strip().lower()\n",
    "        answer = content[6]\n",
    "\n",
    "        if relabeling:\n",
    "            q_words = question.split(' ')\n",
    "            d_words = document.split(' ')\n",
    "            assert answer in d_words\n",
    "\n",
    "            entity_dict = {}\n",
    "            entity_id = 0\n",
    "            for word in d_words + q_words:\n",
    "                if (word.startswith('@entity')) and (word not in entity_dict):\n",
    "                    entity_dict[word] = '@entity' + str(entity_id)\n",
    "                    entity_id += 1\n",
    "\n",
    "            q_words = [entity_dict[w] if w in entity_dict else w for w in q_words]\n",
    "            d_words = [entity_dict[w] if w in entity_dict else w for w in d_words]\n",
    "            answer = entity_dict[answer]\n",
    "\n",
    "            question = ' '.join(q_words)\n",
    "            document = ' '.join(d_words)\n",
    "\n",
    "        questions.append(question)\n",
    "        answers.append(answer)\n",
    "        documents.append(document)\n",
    "        num_examples += 1\n",
    "\n",
    "        if (max_example is not None) and (num_examples >= max_example):\n",
    "            break\n",
    "    print(\"#Examples: {}\".format(len(documents)))\n",
    "    f.close()\n",
    "    \n",
    "    if write_file:\n",
    "        f = open(write_file, 'w')\n",
    "        for i in range(len(questions)):\n",
    "            f.write(documents[i]+\"\\n\")\n",
    "            f.write(questions[i]+\"\\n\")\n",
    "            f.write(answers[i]+\"\\n\")\n",
    "        f.close()\n",
    "    \n",
    "    return (documents, questions, answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Examples: 3198\n"
     ]
    }
   ],
   "source": [
    "documents, questions, answers = create_data_files(test_path), write_file=os.path.join(data_path, \"test.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(in_file_path, max_examples=None):\n",
    "    \"\"\"\n",
    "    load CNN / Daily Mail data from {train | dev | test}.txt\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    num_examples = 0\n",
    "    \n",
    "    f = open(in_file_path, 'r')\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "            \n",
    "        document = line.strip().lower()\n",
    "        question = f.readline().strip()\n",
    "        answer = f.readline().strip().lower()\n",
    "        num_examples += 1\n",
    "        \n",
    "        questions.append(question)\n",
    "        answers.append(answer)\n",
    "        documents.append(document)\n",
    "        \n",
    "        if (max_examples is not None) and (num_examples >= max_examples):\n",
    "            break\n",
    "    print(\"#Examples: {}\".format(len(documents)))\n",
    "    f.close()\n",
    "    return (documents, questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Examples: 100\n"
     ]
    }
   ],
   "source": [
    "documents, questions, answers = load_data(os.path.join(data_path, \"train.txt\"), max_examples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dict(sentences, max_words=50000):\n",
    "    \"\"\"\n",
    "        Build a dictionary for the words in `sentences`.\n",
    "        Only the max_words ones are kept and the remaining will be mapped to <UNK>.\n",
    "    \"\"\"\n",
    "    word_count = Counter()\n",
    "    for sent in sentences:\n",
    "        for w in sent.split(' '):\n",
    "            word_count[w] += 1\n",
    "\n",
    "    ls = word_count.most_common(max_words)\n",
    "\n",
    "    # leave 0 to UNK\n",
    "    # leave 1 to delimiter |||\n",
    "    return {w[0]: index + 2 for (index, w) in enumerate(ls)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary_dict = build_dict(documents+questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entity_markers = list(set([w for w in vocabulary_dict.keys()\n",
    "                              if w.startswith('@entity')]+answers))\n",
    "entity_markers = ['<unk_entity>'] + entity_markers\n",
    "entity_dict = {w: index for (index, w) in enumerate(entity_markers)}\n",
    "num_labels = len(entity_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(examples, word_dict, entity_dict,\n",
    "              sort_by_len=True, verbose=True):\n",
    "    \"\"\"\n",
    "        Vectorize `examples`.\n",
    "        in_x1, in_x2: sequences for document and question respecitvely.\n",
    "        in_y: label\n",
    "        in_l: whether the entity label occurs in the document.\n",
    "    \"\"\"\n",
    "    in_x1 = []\n",
    "    in_x2 = []\n",
    "    in_l = np.zeros((len(examples[0]), len(entity_dict))).astype(config._floatX)\n",
    "    in_y = []\n",
    "    for idx, (d, q, a) in enumerate(zip(examples[0], examples[1], examples[2])):\n",
    "        d_words = d.split(' ')\n",
    "        q_words = q.split(' ')\n",
    "        assert (a in d_words)\n",
    "        seq1 = [word_dict[w] if w in word_dict else 0 for w in d_words]\n",
    "        seq2 = [word_dict[w] if w in word_dict else 0 for w in q_words]\n",
    "        if (len(seq1) > 0) and (len(seq2) > 0):\n",
    "            in_x1.append(seq1)\n",
    "            in_x2.append(seq2)\n",
    "            in_l[idx, [entity_dict[w] for w in d_words if w in entity_dict]] = 1.0\n",
    "            in_y.append(entity_dict[a] if a in entity_dict else 0)\n",
    "        if verbose and (idx % 10000 == 0):\n",
    "            logging.info('Vectorization: processed %d / %d' % (idx, len(examples[0])))\n",
    "\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "\n",
    "    if sort_by_len:\n",
    "        # sort by the document length\n",
    "        sorted_index = len_argsort(in_x1)\n",
    "        in_x1 = [in_x1[i] for i in sorted_index]\n",
    "        in_x2 = [in_x2[i] for i in sorted_index]\n",
    "        in_l = in_l[sorted_index]\n",
    "        in_y = [in_y[i] for i in sorted_index]\n",
    "\n",
    "    return in_x1, in_x2, in_l, in_y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
