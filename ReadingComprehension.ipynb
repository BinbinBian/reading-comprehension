{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-1e6c9a372a99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'config'"
     ]
    }
   ],
   "source": [
    "#import utils\n",
    "import os\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import numpy as np\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = '/Users/kellyzhang/Documents/ReadingComprehension/DeepMindDataset/cnn/questions'\n",
    "train_path = os.path.join(data_path, \"training\") # 380298\n",
    "validation_path = os.path.join(data_path, \"validation\") # 3924\n",
    "test_path = os.path.join(data_path, \"test\") #3198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_data_files(in_file_path, max_example=None, relabeling=True, write_file=None):\n",
    "    \"\"\"\n",
    "    load CNN / Daily Mail data from {train | dev | test} directories\n",
    "    relabeling: relabel the entities by their first occurence if it is True.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    num_examples = 0\n",
    "\n",
    "    for document in os.listdir(in_file_path):\n",
    "        f = open(os.path.join(in_file_path, document), 'r')\n",
    "\n",
    "        content = f.read().splitlines()\n",
    "        document = content[2].strip().lower()\n",
    "        question = content[4].strip().lower()\n",
    "        answer = content[6]\n",
    "\n",
    "        if relabeling:\n",
    "            q_words = question.split(' ')\n",
    "            d_words = document.split(' ')\n",
    "            assert answer in d_words\n",
    "\n",
    "            entity_dict = {}\n",
    "            entity_id = 0\n",
    "            for word in d_words + q_words:\n",
    "                if (word.startswith('@entity')) and (word not in entity_dict):\n",
    "                    entity_dict[word] = '@entity' + str(entity_id)\n",
    "                    entity_id += 1\n",
    "\n",
    "            q_words = [entity_dict[w] if w in entity_dict else w for w in q_words]\n",
    "            d_words = [entity_dict[w] if w in entity_dict else w for w in d_words]\n",
    "            answer = entity_dict[answer]\n",
    "\n",
    "            question = ' '.join(q_words)\n",
    "            document = ' '.join(d_words)\n",
    "\n",
    "        questions.append(question)\n",
    "        answers.append(answer)\n",
    "        documents.append(document)\n",
    "        num_examples += 1\n",
    "\n",
    "        if (max_example is not None) and (num_examples >= max_example):\n",
    "            break\n",
    "    print(\"#Examples: {}\".format(len(documents)))\n",
    "    f.close()\n",
    "    \n",
    "    if write_file:\n",
    "        f = open(write_file, 'w')\n",
    "        for i in range(len(questions)):\n",
    "            f.write(documents[i]+\"\\n\")\n",
    "            f.write(questions[i]+\"\\n\")\n",
    "            f.write(answers[i]+\"\\n\")\n",
    "        f.close()\n",
    "    \n",
    "    return (documents, questions, answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Examples: 3198\n"
     ]
    }
   ],
   "source": [
    "documents, questions, answers = create_data_files(test_path), write_file=os.path.join(data_path, \"test.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(in_file_path, max_examples=None):\n",
    "    \"\"\"\n",
    "    load CNN / Daily Mail data from {train | dev | test}.txt\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    num_examples = 0\n",
    "    \n",
    "    f = open(in_file_path, 'r')\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "            \n",
    "        document = line.strip().lower()\n",
    "        question = f.readline().strip()\n",
    "        answer = f.readline().strip().lower()\n",
    "        num_examples += 1\n",
    "        \n",
    "        questions.append(question)\n",
    "        answers.append(answer)\n",
    "        documents.append(document)\n",
    "        \n",
    "        if (max_examples is not None) and (num_examples >= max_examples):\n",
    "            break\n",
    "    print(\"#Examples: {}\".format(len(documents)))\n",
    "    f.close()\n",
    "    return (documents, questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Examples: 100\n"
     ]
    }
   ],
   "source": [
    "documents, questions, answers = load_data(os.path.join(data_path, \"train.txt\"), max_examples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dict(sentences, max_words=50000):\n",
    "    \"\"\"\n",
    "        Build a dictionary for the words in `sentences`.\n",
    "        Only the max_words ones are kept and the remaining will be mapped to <UNK>.\n",
    "    \"\"\"\n",
    "    word_count = Counter()\n",
    "    for sent in sentences:\n",
    "        for w in sent.split(' '):\n",
    "            word_count[w] += 1\n",
    "\n",
    "    ls = word_count.most_common(max_words)\n",
    "\n",
    "    # leave 0 to UNK\n",
    "    # leave 1 to delimiter |||\n",
    "    return {w[0]: index + 2 for (index, w) in enumerate(ls)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary_dict = build_dict(documents+questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entity_markers = list(set([w for w in vocabulary_dict.keys()\n",
    "                              if w.startswith('@entity')]+answers))\n",
    "entity_markers = ['<unk_entity>'] + entity_markers\n",
    "entity_dict = {w: index for (index, w) in enumerate(entity_markers)}\n",
    "num_labels = len(entity_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gen_embeddings for pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(documents, questions, answers, vocabulary_dict, entity_dict,\n",
    "              sort_by_len=True, verbose=True):\n",
    "    \"\"\"\n",
    "        Vectorize `examples`.\n",
    "        in_d, in_q: sequences for document and question respecitvely.\n",
    "        in_y: label\n",
    "        in_l: whether the entity label occurs in the document.\n",
    "    \"\"\"\n",
    "    in_d = []\n",
    "    in_q = []\n",
    "    in_l = np.zeros((len(answers), len(entity_dict)))\n",
    "    in_y = []\n",
    "    for idx in range(len(answers)):\n",
    "        d_words = documents[idx].split(' ')\n",
    "        q_words = questions[idx].split(' ')\n",
    "        assert (answers[idx] in d_words)\n",
    "        seq1 = [vocabulary_dict[w] if w in vocabulary_dict else 0 for w in d_words]\n",
    "        seq2 = [vocabulary_dict[w] if w in vocabulary_dict else 0 for w in q_words]\n",
    "        if (len(seq1) > 0) and (len(seq2) > 0):\n",
    "            in_d.append(seq1)\n",
    "            in_q.append(seq2)\n",
    "            in_l[idx, [entity_dict[w] for w in d_words if w in entity_dict]] = 1.0\n",
    "            in_y.append(entity_dict[answers[idx]] if answers[idx] in entity_dict else 0)\n",
    "        if verbose and (idx % 10000 == 0):\n",
    "            print('Vectorization: processed {} / {}'.format(idx, len(answers)))\n",
    "\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "\n",
    "    if sort_by_len:\n",
    "        # sort by the document length\n",
    "        sorted_index = len_argsort(in_d)\n",
    "        in_d = [in_d[i] for i in sorted_index]\n",
    "        in_q = [in_q[i] for i in sorted_index]\n",
    "        in_l = in_l[sorted_index]\n",
    "        in_y = [in_y[i] for i in sorted_index]\n",
    "\n",
    "    return in_d, in_q, in_l, in_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization: processed 0 / 100\n"
     ]
    }
   ],
   "source": [
    "a = vectorize(documents, questions, answers, vocabulary_dict, entity_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52, 64, 10, 648, 5, 42, 359, 310, 3, 42, 96, 181, 388, 116]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
